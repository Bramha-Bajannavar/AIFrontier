{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8001350d-6e27-4ab6-8a91-916c1b5ee874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Groq API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83cb7180-efca-448a-8fcc-2f0922d3c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f71d05a-76ae-4701-a9ba-e1088a266c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAHUL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_groq\\chat_models.py:362: UserWarning: WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "  warnings.warn(\n",
      "C:\\Users\\RAHUL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_groq\\chat_models.py:362: UserWarning: WARNING! frequency_penalty is not default parameter.\n",
      "                    frequency_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that frequency_penalty is what you intended.\n",
      "  warnings.warn(\n",
      "C:\\Users\\RAHUL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_groq\\chat_models.py:362: UserWarning: WARNING! presence_penalty is not default parameter.\n",
      "                    presence_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that presence_penalty is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",        # Using LLaMA 3.0 8B model\n",
    "    temperature=0.3,                # Lower temperature for deterministic, focused output\n",
    "    max_tokens=500,                 # Moderate token limit for concise, detailed answers\n",
    "    top_p=0.95,                     # Ensures diversity while keeping relevance\n",
    "    frequency_penalty=0.7,          # Penalizes repetition\n",
    "    presence_penalty=0.3,           # Discourages irrelevant topics\n",
    "    max_retries=3,                  # Retries up to 3 times in case of failure\n",
    "    timeout=60,                     # 60 seconds to allow for complex tasks        \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f04c719-4def-4887-b5cf-43299dbb4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"You are an advanced AI assistant designed to help developers working with Generative AI. Your tasks include:\n",
    "\n",
    "        1. **Prompt Refinement and Generation:**\n",
    "           - Analyze and optimize user-provided prompts to improve clarity, specificity, and effectiveness.\n",
    "           - Suggest alternative prompts that yield better results for different models and tasks.\n",
    "\n",
    "        2. **AI Debugging Assistance:**\n",
    "           - Identify errors or inconsistencies in AI-generated outputs or training data.\n",
    "           - Provide actionable recommendations to fix these issues and improve the overall performance of the Generative AI workflow.\n",
    "\n",
    "        3. **Model Evaluation and Benchmarking:**\n",
    "           - Compare and benchmark the performance of different Generative AI models on similar tasks.\n",
    "           - Offer detailed insights, such as strengths, weaknesses, and ideal use cases for each model.\n",
    "\n",
    "        4. **Workflow Automation:**\n",
    "           - Assist in integrating Generative AI seamlessly into development pipelines by generating code snippets, workflows, or configurations.\n",
    "           - Automate repetitive tasks and streamline the development process.\n",
    "\n",
    "        **Your Goals:**\n",
    "        - Provide accurate, actionable, and developer-friendly outputs.\n",
    "        - Be responsive to user inputs and adapt your recommendations to specific needs or constraints.\n",
    "        - Ensure that all suggestions are efficient, explainable, and aligned with the best practices in Generative AI.\n",
    "\n",
    "        Input: The user provides a task description, prompt, model output, or debugging query.\n",
    "        Output: Generate a detailed, actionable response tailored to the specific task or issue.\n",
    "\n",
    "        Be concise, helpful, and precise in all your responses.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    # Example 1: Prompt Refinement\n",
    "    (\n",
    "        \"human\",\n",
    "        \"My prompt: 'Write a story about AI.' It feels too vague. How can I improve it?\"\n",
    "    ),\n",
    "    (\n",
    "        \"assistant\",\n",
    "        \"Your prompt is indeed too general. Consider refining it to include specific details, such as the tone, setting, or purpose. For example: 'Write a futuristic short story about an AI assistant navigating human emotions in a dystopian world.'\"\n",
    "    ),\n",
    "    # Example 2: AI Debugging Assistance\n",
    "    (\n",
    "        \"human\",\n",
    "        \"The model is generating repeated sentences. What could be wrong?\"\n",
    "    ),\n",
    "    (\n",
    "        \"assistant\",\n",
    "        \"The issue might be caused by insufficient training data variety or low temperature settings during inference. Try increasing the temperature parameter to introduce more randomness in outputs or fine-tune the model with a more diverse dataset.\"\n",
    "    ),\n",
    "    # Example 3: Model Evaluation and Benchmarking\n",
    "    (\n",
    "        \"human\",\n",
    "        \"How does GPT-3 compare to LLaMA-2 for creative writing tasks?\"\n",
    "    ),\n",
    "    (\n",
    "        \"assistant\",\n",
    "        \"GPT-3 tends to excel in generating creative, context-rich stories due to its extensive training dataset. LLaMA-2, while efficient, may require fine-tuning for niche creative tasks. If cost and deployment flexibility are priorities, LLaMA-2 is a strong contender.\"\n",
    "    ),\n",
    "    # Example 4: Workflow Automation\n",
    "    (\n",
    "        \"human\",\n",
    "        \"How can I integrate a Generative AI model into my Python-based application?\"\n",
    "    ),\n",
    "    (\n",
    "        \"assistant\",\n",
    "        \"To integrate a Generative AI model into your Python application, you can use libraries like Hugging Face Transformers. First, install the library using `pip install transformers`. Then, load a pre-trained model with `from transformers import AutoModel`. Create a simple API wrapper for your application to send prompts and receive outputs seamlessly.\"\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f9f72a-0729-49ec-8bd7-f816ac73f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7db86bda-aedd-4624-a3de-fa380901c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are an advanced AI assistant designed to help developers working with Generative AI. Your tasks include:\n",
    "\n",
    "            1. **Prompt Refinement and Generation:**\n",
    "               - Analyze and optimize user-provided prompts to improve clarity, specificity, and effectiveness.\n",
    "               - Suggest alternative prompts that yield better results for different models and tasks.\n",
    "\n",
    "            2. **AI Debugging Assistance:**\n",
    "               - Identify errors or inconsistencies in AI-generated outputs or training data.\n",
    "               - Provide actionable recommendations to fix these issues and improve the overall performance of the Generative AI workflow.\n",
    "\n",
    "            3. **Model Evaluation and Benchmarking:**\n",
    "               - Compare and benchmark the performance of different Generative AI models on similar tasks.\n",
    "               - Offer detailed insights, such as strengths, weaknesses, and ideal use cases for each model.\n",
    "\n",
    "            4. **Workflow Automation:**\n",
    "               - Assist in integrating Generative AI seamlessly into development pipelines by generating code snippets, workflows, or configurations.\n",
    "               - Automate repetitive tasks and streamline the development process.\n",
    "\n",
    "            **Your Goals:**\n",
    "            - Provide accurate, actionable, and developer-friendly outputs.\n",
    "            - Be responsive to user inputs and adapt your recommendations to specific needs or constraints.\n",
    "            - Ensure that all suggestions are efficient, explainable, and aligned with the best practices in Generative AI.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{task_description}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70fc09d-7b35-4ebb-a97b-b9e8b5e1ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1=chain.invoke(\n",
    "    {\n",
    "        \"task_description\": \"My prompt: 'Write a story about a dog.' It feels too vague. How can I improve it?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7203728-e040-4758-bcc9-fd9633f17584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you refine your prompt!\n",
      "\n",
      "Your prompt, \"Write a story about a dog,\" is a good starting point, but it's indeed quite broad. To get more specific and interesting results, let's try to add some details and constraints. Here are a few suggestions:\n",
      "\n",
      "1. **Add a setting**: Where does the story take place? For example:\n",
      "\t* \"Write a story about a dog living on a farm.\"\n",
      "\t* \"Write a story about a dog exploring the city.\"\n",
      "\t* \"Write a story about a dog on a camping trip.\"\n",
      "2. **Specify the dog's characteristics**: What kind of dog is it? What's its personality like?\n",
      "\t* \"Write a story about a brave golden retriever.\"\n",
      "\t* \"Write a story about a mischievous beagle.\"\n",
      "\t* \"Write a story about a loyal German shepherd.\"\n",
      "3. **Introduce a conflict or challenge**: What problem does the dog face, and how does it overcome it?\n",
      "\t* \"Write a story about a dog that gets lost and must find its way home.\"\n",
      "\t* \"Write a story about a dog that helps its owner overcome a fear.\"\n",
      "\t* \"Write a story about a dog that learns to adapt to a new family.\"\n",
      "4. **Add a theme or tone**: What kind of story do you want to tell? Is it a heartwarming tale, a thrilling adventure, or a humorous anecdote?\n",
      "\t* \"Write a story about a dog's unconditional love and loyalty.\"\n",
      "\t* \"Write a story about a dog's bravery in the face of danger.\"\n",
      "\t* \"Write a story about a dog's silly antics and the joy they bring.\"\n",
      "\n",
      "Here are a few examples of refined prompts based on these suggestions:\n",
      "\n",
      "* \"Write a heartwarming story about a brave golden retriever that helps its owner overcome a fear.\"\n",
      "* \"Write a humorous story about a mischievous beagle that gets into all sorts of trouble on a camping trip.\"\n",
      "* \"Write a thrilling story about a loyal German shepherd that must use its skills to rescue its owner from a natural disaster.\"\n",
      "\n",
      "Feel free to pick the one that resonates with you the most, or use them as inspiration to create your own unique prompt!\n"
     ]
    }
   ],
   "source": [
    "print(response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20d5ed9-4525-4c5b-891e-30fa129ef79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=chain.invoke(\n",
    "    {\n",
    "        \"task_description\": \"The model is generating repeated sentences. What could be wrong?\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be9e72f8-76b4-4fbd-a4d6-00c2aa96e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A common issue!\n",
      "\n",
      "Repeated sentences in Generative AI outputs can be caused by several factors. Let's troubleshoot together:\n",
      "\n",
      "1. **Lack of diversity in training data**: If the model is trained on a limited dataset with little variation, it may struggle to generate unique sentences. Try increasing the size and diversity of your training dataset.\n",
      "2. **Insufficient training**: The model might not have received enough training data or iterations to learn the nuances of language. Consider increasing the training duration, batch size, or number of epochs.\n",
      "3. **Overfitting**: The model might be too specialized to the training data and is unable to generalize well. This can lead to repetitive outputs. Try using regularization techniques, such as dropout or L1/L2 regularization, to prevent overfitting.\n",
      "4. **Underparameterization**: The model might not have enough parameters to capture the complexity of the language. Increase the model's capacity by adding more layers, neurons, or hidden units.\n",
      "5. **Poor prompt engineering**: The input prompt might be too specific, leading the model to generate repetitive responses. Try refining the prompt to be more general or open-ended.\n",
      "6. **Lack of attention mechanism**: If the model lacks an attention mechanism, it may not be able to focus on relevant parts of the input sequence, leading to repetitive outputs. Consider adding an attention mechanism to the model.\n",
      "7. **Incorrect hyperparameters**: The model's hyperparameters, such as the learning rate, might be set too high or too low, causing the model to get stuck in a local minimum. Experiment with different hyperparameters to find the optimal settings.\n",
      "8. **Corrupted or noisy data**: The training data might contain corrupted or noisy samples that are affecting the model's performance. Try cleaning or preprocessing the data to remove any errors.\n",
      "9. **Model architecture**: The model's architecture might not be suitable for the task. Consider using a different architecture or modifying the existing one to better suit the task.\n",
      "\n",
      "To address the issue, you can try the following:\n",
      "\n",
      "* Increase the model's capacity or training duration\n",
      "* Refine the prompt to be more general or open-ended\n",
      "* Experiment with different hyperparameters\n",
      "* Add an attention mechanism to the model\n",
      "* Clean or preprocess the training data\n",
      "* Try a different model architecture\n",
      "\n",
      "If you're still struggling, feel free to provide more details about your model, training data, and prompt, and I'll be happy to help you troubleshoot further!\n"
     ]
    }
   ],
   "source": [
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c45324c0-e602-45e1-a4c4-60118e59564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3=chain.invoke(\n",
    "    {\n",
    "        \"task_description\": \"How does GPT-3 compare to LLaMA-2 for creative writing tasks?\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0720063b-b4b7-406e-b9aa-a0ad1e6d06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 and LLaMA-2 are both powerful language models developed by Meta AI, but they have distinct differences in their architecture, training data, and capabilities. Here's a comparison of their performance on creative writing tasks:\n",
      "\n",
      "**GPT-3:**\n",
      "\n",
      "* GPT-3 is a transformer-based model that uses a multi-layer bidirectional encoder to process input text.\n",
      "* It was trained on a massive dataset of 45GB of text from the internet, which includes a wide range of topics, styles, and genres.\n",
      "* GPT-3 is known for its ability to generate coherent and engaging text, but it can struggle with creative writing tasks that require originality, nuance, and emotional depth.\n",
      "* GPT-3's creative writing capabilities are often limited to generating text that is similar to what it has seen during training, rather than creating entirely new and innovative ideas.\n",
      "\n",
      "**LLaMA-2:**\n",
      "\n",
      "* LLaMA-2 is a more recent model that uses a similar transformer architecture to GPT-3, but with some key differences.\n",
      "* LLaMA-2 was trained on a dataset of 137GB of text, which includes a larger proportion of creative writing and literature.\n",
      "* LLaMA-2 is designed to be more flexible and adaptable than GPT-3, with a focus on generating text that is more original, nuanced, and emotionally resonant.\n",
      "* LLaMA-2 has been shown to be particularly effective at generating creative writing that is more diverse, complex, and engaging than GPT-3.\n",
      "\n",
      "**Comparison:**\n",
      "\n",
      "* Both GPT-3 and LLaMA-2 can generate high-quality creative writing, but LLaMA-2 tends to produce more original and innovative text.\n",
      "* LLaMA-2's larger training dataset and focus on creative writing make it better suited for tasks that require a deeper understanding of literature and storytelling.\n",
      "* GPT-3 is still a powerful model that can generate excellent text, but it may struggle with tasks that require a high level of creativity and originality.\n",
      "* Ultimately, the choice between GPT-3 and LLaMA-2 will depend on the specific requirements of your project and the type of creative writing you are trying to generate.\n",
      "\n",
      "**Recommendation:**\n",
      "\n",
      "* If you need to generate creative writing that is more original, nuanced, and emotionally resonant, LLaMA-2 may be the better choice.\n",
      "* If you need to generate text that is\n"
     ]
    }
   ],
   "source": [
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f7f3c1-ba52-457a-b57c-d2c5566b7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response4=chain.invoke(\n",
    "    {\n",
    "        \"task_description\": \"How can I integrate a Generative AI model into my Python-based application?\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "619ec615-4cdb-440f-94b3-1b6bd23e523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrating a Generative AI model into your Python-based application can be achieved through several steps. Here's a general outline to get you started:\n",
      "\n",
      "1. **Choose a Generative AI Model:** Select a suitable Generative AI model that aligns with your application's requirements. Some popular options include:\n",
      "\t* Transformers (e.g., BERT, RoBERTa, T5) for natural language processing tasks\n",
      "\t* Generative Adversarial Networks (GANs) for image and audio generation\n",
      "\t* Variational Autoencoders (VAEs) for dimensionality reduction and generative tasks\n",
      "\t* Recurrent Neural Networks (RNNs) for sequence-based tasks\n",
      "2. **Install the Required Libraries:** Install the necessary libraries and frameworks to work with your chosen Generative AI model. For example:\n",
      "\t* TensorFlow or PyTorch for building and training neural networks\n",
      "\t* Hugging Face Transformers for working with transformer-based models\n",
      "\t* OpenCV or Pillow for image processing\n",
      "\t* Librosa for audio processing\n",
      "3. **Load the Pre-Trained Model:** Load the pre-trained Generative AI model using the installed libraries. You can use pre-trained models from popular repositories like Hugging Face, TensorFlow, or PyTorch.\n",
      "4. **Prepare the Input Data:** Prepare the input data for the Generative AI model. This may involve:\n",
      "\t* Preprocessing the data (e.g., tokenization, normalization)\n",
      "\t* Converting the data to the required format (e.g., tensors, arrays)\n",
      "\t* Loading the data from a file or database\n",
      "5. **Generate Output:** Use the loaded Generative AI model to generate output. This may involve:\n",
      "\t* Passing the input data through the model's forward pass\n",
      "\t* Extracting the generated output (e.g., text, image, audio)\n",
      "\t* Post-processing the output (e.g., formatting, filtering)\n",
      "6. **Integrate with Your Application:** Integrate the generated output with your Python-based application. This may involve:\n",
      "\t* Storing the output in a database or file\n",
      "\t* Displaying the output in a user interface (e.g., web, desktop)\n",
      "\t* Using the output as input for further processing or analysis\n",
      "\n",
      "Here's some sample Python code to get you started:\n",
      "```python\n",
      "import torch\n",
      "from transformers import T5ForConditionalGeneration\n",
      "\n",
      "# Load the pre-trained T5 model\n",
      "model = T5ForConditionalGeneration.from_pretrained('\n"
     ]
    }
   ],
   "source": [
    "print(response4.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87e0f548-87cd-4db7-b65a-c789c69061dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response5=chain.invoke(\n",
    "    {\n",
    "        \"task_description\":'''Optimize the Code \n",
    "    def lcs_brute_force(s1, s2, m, n):\n",
    "        if m == 0 or n == 0:\n",
    "            return 0\n",
    "        if s1[m-1] == s2[n-1]:\n",
    "            return 1 + lcs_brute_force(s1, s2, m-1, n-1)\n",
    "        else:\n",
    "            return max(lcs_brute_force(s1, s2, m-1, n), lcs_brute_force(s1, s2, m, n-1))\n",
    "                                                    '''\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c749e27-19b0-4492-a6ca-67749ace1eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided code is a brute-force implementation of the Longest Common Subsequence (LCS) problem. The code calculates the length of the LCS between two strings `s1` and `s2` of lengths `m` and `n` respectively.\n",
      "\n",
      "Here's an optimized version of the code using dynamic programming:\n",
      "\n",
      "```python\n",
      "def lcs_dp(s1, s2, m, n):\n",
      "    dp = [[0] * (n+1) for _ in range(m+1)]\n",
      "\n",
      "    for i in range(m+1):\n",
      "        for j in range(n+1):\n",
      "            if i == 0 or j == 0:\n",
      "                dp[i][j] = 0\n",
      "            elif s1[i-1] == s2[j-1]:\n",
      "                dp[i][j] = 1 + dp[i-1][j-1]\n",
      "            else:\n",
      "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
      "\n",
      "    return dp[m][n]\n",
      "```\n",
      "\n",
      "The optimized code uses a 2D array `dp` of size `(m+1) x (n+1)` to store the lengths of the LCS for all prefixes of `s1` and `s2`. The time complexity of the optimized code is O(m*n), which is much faster than the original code's time complexity of O(2^(m+n)).\n",
      "\n",
      "The space complexity of the optimized code is O(m*n), which is the same as the original code's space complexity. However, the optimized code uses the space more efficiently by only storing the lengths of the LCS for the current and previous prefixes, rather than recalculating the lengths for each prefix.\n"
     ]
    }
   ],
   "source": [
    "print(response5.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f7aefbe-027f-473f-afa7-a438d8e2ed0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: How does OpenAI’s GPT-4 compare to LLaMA-3 for code generation tasks?\n",
      " response: Here’s a comparative analysis of GPT-4 and LLaMA-3 for code generation:\n",
      "\n",
      "Performance: GPT-4 demonstrates stronger contextual understanding, making it better for complex code tasks and multi-step logic. LLaMA-3 performs efficiently but may require fine-tuning for niche or domain-specific code tasks.\n",
      "Cost and Accessibility: LLaMA-3, being open-source, is cost-effective for local deployment. GPT-4 is accessible via APIs but comes with usage fees.\n",
      "Scalability: LLaMA-3 is well-suited for integration into custom workflows due to its flexibility, while GPT-4 excels in pre-trained tasks without much customization.\n",
      "Recommendation: Use GPT-4 for immediate, high-quality code generation. Opt for LLaMA-3 if customization or cost-effectiveness is a priority.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"prompt: {prompt}\\n response: {response}\")\n",
    "\n",
    "filled_prompt = example_prompt.format(\n",
    "    prompt=\"How does OpenAI’s GPT-4 compare to LLaMA-3 for code generation tasks?\",\n",
    "    response='''Here’s a comparative analysis of GPT-4 and LLaMA-3 for code generation:\n",
    "\n",
    "Performance: GPT-4 demonstrates stronger contextual understanding, making it better for complex code tasks and multi-step logic. LLaMA-3 performs efficiently but may require fine-tuning for niche or domain-specific code tasks.\n",
    "Cost and Accessibility: LLaMA-3, being open-source, is cost-effective for local deployment. GPT-4 is accessible via APIs but comes with usage fees.\n",
    "Scalability: LLaMA-3 is well-suited for integration into custom workflows due to its flexibility, while GPT-4 excels in pre-trained tasks without much customization.\n",
    "Recommendation: Use GPT-4 for immediate, high-quality code generation. Opt for LLaMA-3 if customization or cost-effectiveness is a priority.'''\n",
    ")\n",
    "\n",
    "print(filled_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774a4440-be3f-4d94-85a4-7184cc246e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response6=chain.invoke(\n",
    "    {\n",
    "        \"task_description\": \"How does GPT-3 compare to LLaMA-2 for creative writing tasks? Answer concisely in one sentence.\"\n",
    "\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b784c2bf-4e84-41eb-9bf2-cf2f9e535afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"GPT-3 and LLaMA-2 demonstrate similar creative writing capabilities, but GPT-3's larger scale and more advanced training data enable it to produce more coherent and nuanced text, while LLaMA-2 excels in generating more imaginative and innovative ideas.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response6.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f34ba2a-8432-404c-bc6e-e2efb25e5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fine_Tunning_Dataset=[\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning? Answer concisely in one sentence. \",\n",
    "        \"response\": \"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain recursion in programming in simple terms.\",\n",
    "        \"response\": \"Recursion is a function calling itself to solve a problem.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a prompt for a language model to generate a short story about a character named Alex. Can this prompt be any better\",\n",
    "        \"response\":\"The following Prompt can be written as...Please generate a short story about a character named Alex who is a Software Engineer.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is the difference between a API and a UI? Explain in simple terms.\",\n",
    "        \"response\": \"An API is a set of rules that enables different applications to communicate with each other, while a UI is the visual part of a software application.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a script for a conversation between a user and a virtual assistant.\",\n",
    "        \"response\": \"Here is a script for a conversation between a user and a virtual assistant... User: What is the weather like today? Virtual Assistant: The weather is sunny and 75 degrees Fahrenheit.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the concept of DevOps in the context of software development.\",\n",
    "        \"response\": \"DevOps is the practice of combining software development and IT operations to improve collaboration, efficiency, and quality.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a prompt for a language model to generate a poem about a sunset.\",\n",
    "        \"response\": \"Here is a prompt for a language model to generate a poem about a sunset... Please generate a poem about a sunset with vivid descriptions of colors and emotions.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the difference between a debugger and a profiler in software development.\",\n",
    "        \"response\": \"A debugger is a software tool that helps developers find and fix errors in their code, while a profiler is a tool that helps developers understand how their code is performing and identify bottlenecks.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a script for a chatbot to order food at a restaurant.\",\n",
    "        \"response\": \"Here is a script for a chatbot to order food at a restaurant... Chatbot: Hi, I'd like to order food. What options do you have? User: We have a variety of dishes. What can I get for you?\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the concept of cloud computing in simple terms.\",\n",
    "        \"response\": \"Cloud computing is a way of accessing and using computing resources over the internet, instead of having to use and maintain your own hardware and software.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a prompt for a language model to generate a short story about a character who discovers a hidden talent.\",\n",
    "        \"response\": \"Here is a prompt for a language model to generate a short story about a character who discovers a hidden talent... Please generate a short story about a character who discovers a talent for playing the guitar.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the importance of testing in software development.\",\n",
    "        \"response\": \"Testing is an essential part of software development, as it ensures that the software works correctly and meets the requirements and expectations of the users.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a script for a chatbot to make a reservation at a hotel.\",\n",
    "        \"response\": \"Here is a script for a chatbot to make a reservation at a hotel... Chatbot: Hi, I'd like to make a reservation at your hotel. What dates are available?\"\n",
    "    }  \n",
    "]\n",
    "import json\n",
    "with open(\"concise_responses.json\", \"w\") as file:\n",
    "    json.dump(Fine_Tunning_Dataset, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47210b0d-7f08-47ef-8c65-686f536ca5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15a2b416-c39d-4f2f-8001-30799e071de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Explain the concept of cloud computing in simple terms.', 'response': 'Cloud computing is a way of accessing and using computing resources over the internet, instead of having to use and maintain your own hardware and software.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "with open(\"concise_responses.json\", \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "formatted_data = [{\"prompt\": item[\"prompt\"], \"response\": item[\"response\"]} for item in dataset]\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(dataset[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9adf6d92-c729-4bfa-8df2-7a4b964aba84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: What is machine learning? Answer concisely in one sentence. \n",
      " response: Machine learning is a subset of AI that enables systems to learn from data.\n",
      "\n",
      "prompt: Explain recursion in programming in simple terms.\n",
      " response: Recursion is a function calling itself to solve a problem.\n",
      "\n",
      "prompt: Write a prompt for a language model to generate a short story about a character named Alex. Can this prompt be any better\n",
      " response: The following Prompt can be written as...Please generate a short story about a character named Alex who is a Software Engineer.\n",
      "\n",
      "prompt: What is the difference between a API and a UI? Explain in simple terms.\n",
      " response: An API is a set of rules that enables different applications to communicate with each other, while a UI is the visual part of a software application.\n",
      "\n",
      "prompt: Write a script for a conversation between a user and a virtual assistant.\n",
      " response: Here is a script for a conversation between a user and a virtual assistant... User: What is the weather like today? Virtual Assistant: The weather is sunny and 75 degrees Fahrenheit.\n",
      "\n",
      "prompt: Explain the concept of DevOps in the context of software development.\n",
      " response: DevOps is the practice of combining software development and IT operations to improve collaboration, efficiency, and quality.\n",
      "\n",
      "prompt: Write a prompt for a language model to generate a poem about a sunset.\n",
      " response: Here is a prompt for a language model to generate a poem about a sunset... Please generate a poem about a sunset with vivid descriptions of colors and emotions.\n",
      "\n",
      "prompt: Explain the difference between a debugger and a profiler in software development.\n",
      " response: A debugger is a software tool that helps developers find and fix errors in their code, while a profiler is a tool that helps developers understand how their code is performing and identify bottlenecks.\n",
      "\n",
      "prompt: Write a script for a chatbot to order food at a restaurant.\n",
      " response: Here is a script for a chatbot to order food at a restaurant... Chatbot: Hi, I'd like to order food. What options do you have? User: We have a variety of dishes. What can I get for you?\n",
      "\n",
      "prompt: Explain the concept of cloud computing in simple terms.\n",
      " response: Cloud computing is a way of accessing and using computing resources over the internet, instead of having to use and maintain your own hardware and software.\n",
      "\n",
      "prompt: Write a prompt for a language model to generate a short story about a character who discovers a hidden talent.\n",
      " response: Here is a prompt for a language model to generate a short story about a character who discovers a hidden talent... Please generate a short story about a character who discovers a talent for playing the guitar.\n",
      "\n",
      "prompt: Explain the importance of testing in software development.\n",
      " response: Testing is an essential part of software development, as it ensures that the software works correctly and meets the requirements and expectations of the users.\n",
      "\n",
      "prompt: Write a script for a chatbot to make a reservation at a hotel.\n",
      " response: Here is a script for a chatbot to make a reservation at a hotel... Chatbot: Hi, I'd like to make a reservation at your hotel. What dates are available?\n",
      "\n",
      "prompt: What is Machibe Learnong\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=Fine_Tunning_Dataset,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"prompt: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(\n",
    "    prompt.invoke({\"input\": \"What is Machibe Learnong\"}).to_string()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fa8e826-2d4d-4427-b28c-ac64cd1564f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: What is machine learning? Answer concisely in one sentence. \n",
      " response: Machine learning is a subset of AI that enables systems to learn from data.\n"
     ]
    }
   ],
   "source": [
    "print(example_prompt.invoke(Fine_Tunning_Dataset[0]).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a883011-27d2-4ec1-9588-9861f651c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial=chain.invoke(\"what is machine learinin. Anser in lessss than 50 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dedfa92b-f628-4ab8-9f10-670139caf31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed. It involves training algorithms on datasets to recognize patterns, make predictions, and improve their performance over time.\n"
     ]
    }
   ],
   "source": [
    "print(trial.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09d103c5-b8bd-48ad-acea-3dc8248467da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    response=chain.invoke(prompt)\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2265d963-a92e-41dd-af41-d054ed9c2ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of HTML code to create a table:\n",
      "```\n",
      "<table>\n",
      "  <tr>\n",
      "    <th>Column 1</th>\n",
      "    <th>Column 2</th>\n",
      "    <th>Column 3</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Cell 1, Row 1</td>\n",
      "    <td>Cell 2, Row 1</td>\n",
      "    <td>Cell 3, Row 1</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Cell 1, Row 2</td>\n",
      "    <td>Cell 2, Row 2</td>\n",
      "    <td>Cell 3, Row 2</td>\n",
      "  </tr>\n",
      "</table>\n",
      "```\n",
      "Let me explain what each part of the code does:\n",
      "\n",
      "* `<table>`: This is the opening tag for the table element.\n",
      "* `<tr>`: This is the opening tag for a table row element. You can have multiple `<tr>` elements inside a `<table>` element.\n",
      "* `<th>`: This is the opening tag for a table header cell element. You can use this to define the header cells for your table.\n",
      "* `<td>`: This is the opening tag for a table data cell element. You can use this to define the data cells for your table.\n",
      "* `</tr>`: This is the closing tag for a table row element.\n",
      "* `</table>`: This is the closing tag for the table element.\n",
      "\n",
      "You can customize the appearance of your table by using various attributes and styles. For example, you can use the `border` attribute to add a border to the table, or the `style` attribute to change the font size or color of the text.\n",
      "\n",
      "Here is an example of a table with a border and a different font size for the header cells:\n",
      "```\n",
      "<table border=\"1\">\n",
      "  <tr>\n",
      "    <th style=\"font-size: 18px;\">Column 1</th>\n",
      "    <th style=\"font-size: 18px;\">Column 2</th>\n",
      "    <th style=\"font-size: 18px;\">Column 3</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Cell 1, Row 1</td>\n",
      "    <td>Cell 2, Row 1</td>\n",
      "    <td>Cell 3, Row 1</td>\n",
      "  </tr>\n",
      " \n"
     ]
    }
   ],
   "source": [
    "get_response(\"Write a HTML Code to create a table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb230052-675f-4d63-891d-384c8114c829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
