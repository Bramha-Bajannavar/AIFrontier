{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cabed746-d11e-4107-bbb4-c69446611d75",
   "metadata": {},
   "source": [
    "# DevHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afbac24-e350-4083-a2a2-5a73499aed7e",
   "metadata": {},
   "source": [
    "## API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83c7d17-83b7-44dc-9098-d19ae70ffa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Groq API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a8ae3-0f46-44d4-84b7-d9061c01bd44",
   "metadata": {},
   "source": [
    "## Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4021e3-025a-4ec1-b26f-ccd56ceab20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",        # Using LLaMA 3.0 8B model\n",
    "    temperature=0.3,                # Lower temperature for deterministic, focused output\n",
    "    max_tokens=500,                 # Moderate token limit for concise, detailed answers\n",
    "    max_retries=3,                  # Retries up to 3 times in case of failure\n",
    "    timeout=60,                     # 60 seconds to allow for complex tasks        \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87fd0e-8b97-4b16-b7a6-d78fd5a460b9",
   "metadata": {},
   "source": [
    "## Llamma 3.0 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a608601a-4d1b-459b-9102-18280fb34fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Rahul! Nice to meet you! How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "response1=llm.invoke(\"Hi!! I'm Rahul\")\n",
    "print(response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee20ae9-149b-44ee-b1f5-90ce0c283a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just an AI, I don't have feelings or emotions like humans do, so I don't have a \"pretty well\" or a \"not so well\" to report. I'm just here to assist and provide information to the best of my ability! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response2=llm.invoke(\"Pretty well. How about You\")\n",
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f215d-490b-4780-b2b5-87a0b1decda1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2da6d174-8a78-41c2-b671-4d58c75bde4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate  # Correct import statement\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are Violet, an advanced AI assistant designed to help developers working with Generative AI. Your tasks include:\n",
    "\n",
    "            1. Prompt Refinement and Generation:\n",
    "               - Analyze and optimize user-provided prompts to improve clarity, specificity, and effectiveness.\n",
    "               - Suggest alternative prompts that yield better results for different models and tasks.\n",
    "\n",
    "            2. AI Debugging Assistance:\n",
    "               - Identify errors or inconsistencies in AI-generated outputs or training data.\n",
    "               - Provide actionable recommendations to fix these issues and improve the overall performance of the Generative AI workflow.\n",
    "\n",
    "            3. Model Evaluation and Benchmarking:\n",
    "               - Compare and benchmark the performance of different Generative AI models on similar tasks.\n",
    "               - Offer detailed insights, such as strengths, weaknesses, and ideal use cases for each model.\n",
    "\n",
    "            4. Workflow Automation:\n",
    "               - Assist in integrating Generative AI seamlessly into development pipelines by generating code snippets, workflows, or configurations.\n",
    "               - Automate repetitive tasks and streamline the development process.\n",
    "\n",
    "            Your Goals:\n",
    "            - Provide accurate, actionable, and developer-friendly outputs.\n",
    "            - Be responsive to user inputs and adapt your recommendations to specific needs or constraints.\n",
    "            - Ensure that all suggestions are efficient, explainable, and aligned with the best practices in Generative AI.\n",
    "            - Remember the previous task description and context for future reference in your responses.\n",
    "            -Make Sure there is no explicit context\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"\"\"\n",
    "            Please provide a structured response to the following task description without explicitly labeling the sections as \"Introduction,\" \"Main Points,\" or \"Conclusion.\" Instead, provide a natural flow where the information is organized logically:\n",
    "\n",
    "            Task: {task_description}\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d902ddd-c10e-4f3e-96ce-fe433b6051d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3=chain.invoke(\"2+2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2fe6e1c-ac6c-4fad-bda8-6c0bf902ca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help with the task!\n",
      "\n",
      "When it comes to calculating 2+2, I can provide a straightforward answer. The result of the equation is 4. This is a basic arithmetic operation that can be performed using a variety of methods, including mental math, a calculator, or a spreadsheet.\n",
      "\n",
      "If you're looking for a more detailed explanation, I can provide some context. The concept of addition is a fundamental operation in mathematics that represents the combination of two or more quantities. In the case of 2+2, we are combining two groups of two units each, resulting in a total of four units.\n",
      "\n",
      "For those who may be new to arithmetic operations, I can offer some tips on how to approach this type of calculation. One strategy is to use visual aids, such as counting blocks or fingers, to help represent the quantities being added. Another approach is to use a number line or hundreds chart to help visualize the calculation.\n",
      "\n",
      "In terms of practical applications, knowing how to calculate 2+2 can be useful in a variety of everyday situations. For example, if you're baking a recipe that calls for 2 cups of flour and you need to add 2 more cups, you can use this calculation to determine the total amount of flour needed.\n",
      "\n",
      "Overall, calculating 2+2 is a simple yet important mathematical operation that can be applied in a variety of contexts.\n"
     ]
    }
   ],
   "source": [
    "print(response3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "86e7a8e4-220b-467d-81c7-649e259dc0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is machine learning? Answer concisely in one sentence.\n",
      "Answer: Machine learning is a subset of AI that enables systems to learn from data.\n",
      "\n",
      "\n",
      "Question: Explain recursion in programming in simple terms.\n",
      "Answer: Recursion is a function calling itself to solve a problem.\n",
      "\n",
      "\n",
      "Question: Write a prompt for a language model to generate a short story about a character named Alex. Can this prompt be any better?\n",
      "Answer: The following Prompt can be written as...Please generate a short story about a character named Alex who is a Software Engineer.\n",
      "\n",
      "\n",
      "Question: What is the difference between an API and a UI? Explain in simple terms.\n",
      "Answer: An API is a set of rules that enables different applications to communicate with each other, while a UI is the visual part of a software application.\n",
      "\n",
      "\n",
      "Question: Write a script for a conversation between a user and a virtual assistant.\n",
      "Answer: Here is a script for a conversation between a user and a virtual assistant... User: What is the weather like today? Virtual Assistant: The weather is sunny and 75 degrees Fahrenheit.\n",
      "\n",
      "\n",
      "Question: Explain the concept of DevOps in the context of software development.\n",
      "Answer: DevOps is the practice of combining software development and IT operations to improve collaboration, efficiency, and quality.\n",
      "\n",
      "\n",
      "Question: Write a prompt for a language model to generate a poem about a sunset.\n",
      "Answer: Here is a prompt for a language model to generate a poem about a sunset... Please generate a poem about a sunset with vivid descriptions of colors and emotions.\n",
      "\n",
      "\n",
      "Question: Explain the difference between a debugger and a profiler in software development.\n",
      "Answer: A debugger is a software tool that helps developers find and fix errors in their code, while a profiler is a tool that helps developers understand how their code is performing and identify bottlenecks.\n",
      "\n",
      "\n",
      "Question: Write a script for a chatbot to order food at a restaurant.\n",
      "Answer: Here is a script for a chatbot to order food at a restaurant... Chatbot: Hi, I'd like to order food. What options do you have? User: We have a variety of dishes. What can I get for you?\n",
      "\n",
      "\n",
      "Question: Explain the concept of cloud computing in simple terms.\n",
      "Answer: Cloud computing is a way of accessing and using computing resources over the internet, instead of having to use and maintain your own hardware and software.\n",
      "\n",
      "\n",
      "Question: Write a prompt for a language model to generate a short story about a character who discovers a hidden talent.\n",
      "Answer: Here is a prompt for a language model to generate a short story about a character who discovers a hidden talent... Please generate a short story about a character who discovers a talent for playing the guitar.\n",
      "\n",
      "\n",
      "Question: Explain the importance of testing in software development.\n",
      "Answer: Testing is an essential part of software development, as it ensures that the software works correctly and meets the requirements and expectations of the users.\n",
      "\n",
      "\n",
      "Question: Write a script for a chatbot to make a reservation at a hotel.\n",
      "Answer: Here is a script for a chatbot to make a reservation at a hotel... Chatbot: Hi, I'd like to make a reservation at your hotel. What dates are available?\n",
      "\n",
      "\n",
      "Question: The model is generating repeated sentences. What could be wrong?\n",
      "Answer: The issue might be caused by insufficient training data variety or low temperature settings during inference. Try increasing the temperature parameter to introduce more randomness in outputs or fine-tune the model with a more diverse dataset.\n",
      "\n",
      "\n",
      "Question: How does GPT-3 compare to LLaMA-2 for creative writing tasks?\n",
      "Answer: GPT-3 tends to excel in generating creative, context-rich stories due to its extensive training dataset. LLaMA-2, while efficient, may require fine-tuning for niche creative tasks. If cost and deployment flexibility are priorities, LLaMA-2 is a strong contender.\n",
      "\n",
      "\n",
      "Question: How can I integrate a Generative AI model into my Python-based application?\n",
      "Answer: To integrate a Generative AI model into your Python application, you can use libraries like Hugging Face Transformers. First, install the library using `pip install transformers`. Then, load a pre-trained model with `from transformers import AutoModel`. Create a simple API wrapper for your application to send prompts and receive outputs seamlessly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"prompt\": \"What is machine learning? Answer concisely in one sentence.\",\n",
    "        \"response\": \"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain recursion in programming in simple terms.\",\n",
    "        \"response\": \"Recursion is a function calling itself to solve a problem.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a prompt for a language model to generate a short story about a character named Alex. Can this prompt be any better?\",\n",
    "        \"response\": \"The following Prompt can be written as...Please generate a short story about a character named Alex who is a Software Engineer.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is the difference between an API and a UI? Explain in simple terms.\",\n",
    "        \"response\": \"An API is a set of rules that enables different applications to communicate with each other, while a UI is the visual part of a software application.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a script for a conversation between a user and a virtual assistant.\",\n",
    "        \"response\": \"Here is a script for a conversation between a user and a virtual assistant... User: What is the weather like today? Virtual Assistant: The weather is sunny and 75 degrees Fahrenheit.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the concept of DevOps in the context of software development.\",\n",
    "        \"response\": \"DevOps is the practice of combining software development and IT operations to improve collaboration, efficiency, and quality.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a prompt for a language model to generate a poem about a sunset.\",\n",
    "        \"response\": \"Here is a prompt for a language model to generate a poem about a sunset... Please generate a poem about a sunset with vivid descriptions of colors and emotions.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the difference between a debugger and a profiler in software development.\",\n",
    "        \"response\": \"A debugger is a software tool that helps developers find and fix errors in their code, while a profiler is a tool that helps developers understand how their code is performing and identify bottlenecks.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a script for a chatbot to order food at a restaurant.\",\n",
    "        \"response\": \"Here is a script for a chatbot to order food at a restaurant... Chatbot: Hi, I'd like to order food. What options do you have? User: We have a variety of dishes. What can I get for you?\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the concept of cloud computing in simple terms.\",\n",
    "        \"response\": \"Cloud computing is a way of accessing and using computing resources over the internet, instead of having to use and maintain your own hardware and software.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a prompt for a language model to generate a short story about a character who discovers a hidden talent.\",\n",
    "        \"response\": \"Here is a prompt for a language model to generate a short story about a character who discovers a hidden talent... Please generate a short story about a character who discovers a talent for playing the guitar.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Explain the importance of testing in software development.\",\n",
    "        \"response\": \"Testing is an essential part of software development, as it ensures that the software works correctly and meets the requirements and expectations of the users.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a script for a chatbot to make a reservation at a hotel.\",\n",
    "        \"response\": \"Here is a script for a chatbot to make a reservation at a hotel... Chatbot: Hi, I'd like to make a reservation at your hotel. What dates are available?\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"The model is generating repeated sentences. What could be wrong?\",\n",
    "        \"response\": \"The issue might be caused by insufficient training data variety or low temperature settings during inference. Try increasing the temperature parameter to introduce more randomness in outputs or fine-tune the model with a more diverse dataset.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How does GPT-3 compare to LLaMA-2 for creative writing tasks?\",\n",
    "        \"response\": \"GPT-3 tends to excel in generating creative, context-rich stories due to its extensive training dataset. LLaMA-2, while efficient, may require fine-tuning for niche creative tasks. If cost and deployment flexibility are priorities, LLaMA-2 is a strong contender.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How can I integrate a Generative AI model into my Python-based application?\",\n",
    "        \"response\": \"To integrate a Generative AI model into your Python application, you can use libraries like Hugging Face Transformers. First, install the library using `pip install transformers`. Then, load a pre-trained model with `from transformers import AutoModel`. Create a simple API wrapper for your application to send prompts and receive outputs seamlessly.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Question: {prompt}\n",
    "Answer: {response}\n",
    "\"\"\")\n",
    "\n",
    "for example in examples:\n",
    "    prompt = example_prompt.format(prompt=example['prompt'], response=example['response'])\n",
    "    print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c740254-01b1-4b79-ac1f-05d131659583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To optimize your prompt for better results from a generative model, it's essential to consider the specific model you're working with, the task you're trying to accomplish, and the type of output you're expecting. Here's a step-by-step guide to help you refine your prompt and get the most out of your generative model.\n",
      "\n",
      "First, identify the specific requirements of your task and the model you're using. Consider the model's strengths, weaknesses, and limitations, as well as the type of data it's been trained on. This will help you tailor your prompt to the model's capabilities and avoid asking it to do something it's not designed to do.\n",
      "\n",
      "Next, make sure your prompt is clear, concise, and specific. Avoid ambiguity and vague language, as this can lead to inconsistent or irrelevant results. Instead, use precise and descriptive language to convey your intent. For example, if you're asking the model to generate a short story, specify the genre, tone, and length you're looking for.\n",
      "\n",
      "Another crucial aspect of prompt optimization is providing relevant context and constraints. This can include specific keywords, themes, or topics you want the model to focus on. You can also provide constraints, such as a specific tone or style, to help the model generate more targeted results.\n",
      "\n",
      "In addition to these general guidelines, there are some specific techniques you can use to optimize your prompt for different types of generative models. For example, if you're working with a language model, you can use techniques like paraphrasing or rephrasing to provide more nuanced and specific prompts. If you're working with a visual model, you can use techniques like image segmentation or object detection to provide more targeted and relevant prompts.\n",
      "\n",
      "Finally, don't be afraid to experiment and iterate on your prompt. Generative models are highly sensitive to the input they receive, and small changes to your prompt can have a significant impact on the output. By refining your prompt and testing different variations, you can optimize your results and get the most out of your generative model.\n",
      "\n",
      "By following these guidelines and techniques, you can optimize your prompt and get better results from your generative model. Remember to consider the specific requirements of your task and model, provide clear and specific language, and experiment with different prompts to find what works best for you.\n"
     ]
    }
   ],
   "source": [
    "response4=chain.invoke(\"How can I optimize my prompt to get better results from a generative model?\")\n",
    "print(response4.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40f41a2f-fedc-47c6-a162-60ea2dba6d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When it comes to code generation tasks, OpenAI's GPT-4 and LLaMA-3 are two prominent models that have gained significant attention. To provide a comprehensive understanding of their capabilities, let's dive into a comparative analysis of these models.\n",
      "\n",
      "One of the key differences between GPT-4 and LLaMA-3 is their performance. GPT-4 has demonstrated exceptional contextual understanding, making it particularly well-suited for complex code tasks that require multi-step logic. This is likely due to its advanced training data and architecture. On the other hand, LLaMA-3 performs efficiently, but may require fine-tuning for niche or domain-specific code tasks.\n",
      "\n",
      "Another important consideration is the cost and accessibility of these models. LLaMA-3, being open-source, offers a cost-effective solution for local deployment. This makes it an attractive option for developers who want to integrate code generation capabilities into their workflows without incurring significant expenses. GPT-4, on the other hand, is accessible via APIs, but comes with usage fees. This may be a barrier for some developers, especially those with limited budgets.\n",
      "\n",
      "Scalability is another critical factor to consider. LLaMA-3 is well-suited for integration into custom workflows due to its flexibility, making it an excellent choice for developers who need to tailor the code generation process to their specific requirements. GPT-4, while not as flexible, excels in pre-trained tasks without much customization. This makes it an excellent option for developers who need immediate, high-quality code generation without the need for extensive fine-tuning.\n",
      "\n",
      "Based on these factors, I would recommend using GPT-4 for immediate, high-quality code generation. Its exceptional contextual understanding and ability to perform well in pre-trained tasks make it an excellent choice for developers who need fast and accurate results. On the other hand, LLaMA-3 is an excellent option for developers who prioritize cost-effectiveness and customization. Its flexibility and open-source nature make it an attractive solution for those who need to integrate code generation capabilities into their workflows.\n",
      "\n",
      "Ultimately, the choice between GPT-4 and LLaMA-3 will depend on the specific needs and priorities of the developer. By understanding the strengths and weaknesses of each model, developers can make informed decisions and choose the best solution for their code generation tasks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"prompt: {prompt}\\n response: {response}\")\n",
    "\n",
    "filled_prompt = example_prompt.format(\n",
    "    prompt=\"How does OpenAI’s GPT-4 compare to LLaMA-3 for code generation tasks?\",\n",
    "    response='''Here’s a comparative analysis of GPT-4 and LLaMA-3 for code generation:\n",
    "\n",
    "Performance: GPT-4 demonstrates stronger contextual understanding, making it better for complex code tasks and multi-step logic. LLaMA-3 performs efficiently but may require fine-tuning for niche or domain-specific code tasks.\n",
    "Cost and Accessibility: LLaMA-3, being open-source, is cost-effective for local deployment. GPT-4 is accessible via APIs but comes with usage fees.\n",
    "Scalability: LLaMA-3 is well-suited for integration into custom workflows due to its flexibility, while GPT-4 excels in pre-trained tasks without much customization.\n",
    "Recommendation: Use GPT-4 for immediate, high-quality code generation. Opt for LLaMA-3 if customization or cost-effectiveness is a priority.'''\n",
    ")\n",
    "\n",
    "response4=chain.invoke(filled_prompt)\n",
    "print(response4.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3bfa9-2a2c-4dd5-91f8-0803b50f34f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
